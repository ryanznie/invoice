{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 2490,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.040160642570281124,
      "grad_norm": 1.0997354984283447,
      "learning_rate": 9.963855421686748e-05,
      "loss": 1.1079,
      "step": 10
    },
    {
      "epoch": 0.08032128514056225,
      "grad_norm": 1.4330190420150757,
      "learning_rate": 9.923694779116465e-05,
      "loss": 0.9012,
      "step": 20
    },
    {
      "epoch": 0.12048192771084337,
      "grad_norm": 1.5424600839614868,
      "learning_rate": 9.883534136546186e-05,
      "loss": 0.6681,
      "step": 30
    },
    {
      "epoch": 0.1606425702811245,
      "grad_norm": 1.140687346458435,
      "learning_rate": 9.843373493975903e-05,
      "loss": 0.4703,
      "step": 40
    },
    {
      "epoch": 0.20080321285140562,
      "grad_norm": 0.8092164397239685,
      "learning_rate": 9.803212851405623e-05,
      "loss": 0.2669,
      "step": 50
    },
    {
      "epoch": 0.24096385542168675,
      "grad_norm": 0.18314705789089203,
      "learning_rate": 9.763052208835341e-05,
      "loss": 0.196,
      "step": 60
    },
    {
      "epoch": 0.28112449799196787,
      "grad_norm": 0.11601331830024719,
      "learning_rate": 9.722891566265061e-05,
      "loss": 0.1372,
      "step": 70
    },
    {
      "epoch": 0.321285140562249,
      "grad_norm": 0.5539779663085938,
      "learning_rate": 9.682730923694779e-05,
      "loss": 0.2032,
      "step": 80
    },
    {
      "epoch": 0.3614457831325301,
      "grad_norm": 0.24046380817890167,
      "learning_rate": 9.642570281124499e-05,
      "loss": 0.134,
      "step": 90
    },
    {
      "epoch": 0.40160642570281124,
      "grad_norm": 0.34604600071907043,
      "learning_rate": 9.602409638554217e-05,
      "loss": 0.1366,
      "step": 100
    },
    {
      "epoch": 0.44176706827309237,
      "grad_norm": 0.2568223476409912,
      "learning_rate": 9.562248995983937e-05,
      "loss": 0.1286,
      "step": 110
    },
    {
      "epoch": 0.4819277108433735,
      "grad_norm": 0.2686197757720947,
      "learning_rate": 9.522088353413655e-05,
      "loss": 0.2045,
      "step": 120
    },
    {
      "epoch": 0.5220883534136547,
      "grad_norm": 0.14388985931873322,
      "learning_rate": 9.481927710843373e-05,
      "loss": 0.1354,
      "step": 130
    },
    {
      "epoch": 0.5622489959839357,
      "grad_norm": 0.1617114096879959,
      "learning_rate": 9.441767068273092e-05,
      "loss": 0.1486,
      "step": 140
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 0.13776803016662598,
      "learning_rate": 9.401606425702811e-05,
      "loss": 0.1655,
      "step": 150
    },
    {
      "epoch": 0.642570281124498,
      "grad_norm": 0.16245731711387634,
      "learning_rate": 9.36144578313253e-05,
      "loss": 0.195,
      "step": 160
    },
    {
      "epoch": 0.6827309236947792,
      "grad_norm": 0.21531030535697937,
      "learning_rate": 9.321285140562249e-05,
      "loss": 0.1654,
      "step": 170
    },
    {
      "epoch": 0.7228915662650602,
      "grad_norm": 0.22920925915241241,
      "learning_rate": 9.281124497991968e-05,
      "loss": 0.1836,
      "step": 180
    },
    {
      "epoch": 0.7630522088353414,
      "grad_norm": 0.2796378433704376,
      "learning_rate": 9.240963855421687e-05,
      "loss": 0.1209,
      "step": 190
    },
    {
      "epoch": 0.8032128514056225,
      "grad_norm": 0.4657916724681854,
      "learning_rate": 9.200803212851406e-05,
      "loss": 0.1212,
      "step": 200
    },
    {
      "epoch": 0.8433734939759037,
      "grad_norm": 0.12807247042655945,
      "learning_rate": 9.160642570281125e-05,
      "loss": 0.1236,
      "step": 210
    },
    {
      "epoch": 0.8835341365461847,
      "grad_norm": 0.12265342473983765,
      "learning_rate": 9.120481927710844e-05,
      "loss": 0.1005,
      "step": 220
    },
    {
      "epoch": 0.9236947791164659,
      "grad_norm": 0.1255730539560318,
      "learning_rate": 9.080321285140563e-05,
      "loss": 0.1135,
      "step": 230
    },
    {
      "epoch": 0.963855421686747,
      "grad_norm": 0.25399622321128845,
      "learning_rate": 9.040160642570282e-05,
      "loss": 0.1212,
      "step": 240
    },
    {
      "epoch": 1.0040160642570282,
      "grad_norm": 0.24297946691513062,
      "learning_rate": 9e-05,
      "loss": 0.1012,
      "step": 250
    },
    {
      "epoch": 1.0441767068273093,
      "grad_norm": 0.23046301305294037,
      "learning_rate": 8.95983935742972e-05,
      "loss": 0.0772,
      "step": 260
    },
    {
      "epoch": 1.0843373493975903,
      "grad_norm": 0.2464030385017395,
      "learning_rate": 8.919678714859438e-05,
      "loss": 0.0907,
      "step": 270
    },
    {
      "epoch": 1.1244979919678715,
      "grad_norm": 0.18668003380298615,
      "learning_rate": 8.879518072289157e-05,
      "loss": 0.0928,
      "step": 280
    },
    {
      "epoch": 1.1646586345381527,
      "grad_norm": 0.25960659980773926,
      "learning_rate": 8.839357429718876e-05,
      "loss": 0.0934,
      "step": 290
    },
    {
      "epoch": 1.2048192771084336,
      "grad_norm": 0.36089345812797546,
      "learning_rate": 8.799196787148595e-05,
      "loss": 0.0774,
      "step": 300
    },
    {
      "epoch": 1.2449799196787148,
      "grad_norm": 0.18523791432380676,
      "learning_rate": 8.759036144578313e-05,
      "loss": 0.07,
      "step": 310
    },
    {
      "epoch": 1.285140562248996,
      "grad_norm": 0.40367916226387024,
      "learning_rate": 8.718875502008033e-05,
      "loss": 0.0702,
      "step": 320
    },
    {
      "epoch": 1.3253012048192772,
      "grad_norm": 0.5303566455841064,
      "learning_rate": 8.67871485943775e-05,
      "loss": 0.0678,
      "step": 330
    },
    {
      "epoch": 1.3654618473895583,
      "grad_norm": 0.1835150420665741,
      "learning_rate": 8.638554216867471e-05,
      "loss": 0.0695,
      "step": 340
    },
    {
      "epoch": 1.4056224899598393,
      "grad_norm": 0.18008920550346375,
      "learning_rate": 8.598393574297188e-05,
      "loss": 0.0487,
      "step": 350
    },
    {
      "epoch": 1.4457831325301205,
      "grad_norm": 0.9325887560844421,
      "learning_rate": 8.558232931726909e-05,
      "loss": 0.0945,
      "step": 360
    },
    {
      "epoch": 1.4859437751004017,
      "grad_norm": 0.1023697555065155,
      "learning_rate": 8.518072289156626e-05,
      "loss": 0.0588,
      "step": 370
    },
    {
      "epoch": 1.5261044176706826,
      "grad_norm": 0.2145661562681198,
      "learning_rate": 8.477911646586347e-05,
      "loss": 0.0508,
      "step": 380
    },
    {
      "epoch": 1.5662650602409638,
      "grad_norm": 0.25670406222343445,
      "learning_rate": 8.437751004016064e-05,
      "loss": 0.061,
      "step": 390
    },
    {
      "epoch": 1.606425702811245,
      "grad_norm": 0.3334185779094696,
      "learning_rate": 8.397590361445784e-05,
      "loss": 0.0471,
      "step": 400
    },
    {
      "epoch": 1.6465863453815262,
      "grad_norm": 0.1109081506729126,
      "learning_rate": 8.357429718875502e-05,
      "loss": 0.054,
      "step": 410
    },
    {
      "epoch": 1.6867469879518073,
      "grad_norm": 0.21703454852104187,
      "learning_rate": 8.317269076305221e-05,
      "loss": 0.0531,
      "step": 420
    },
    {
      "epoch": 1.7269076305220885,
      "grad_norm": 0.44986769556999207,
      "learning_rate": 8.27710843373494e-05,
      "loss": 0.0423,
      "step": 430
    },
    {
      "epoch": 1.7670682730923695,
      "grad_norm": 0.3799237608909607,
      "learning_rate": 8.236947791164659e-05,
      "loss": 0.0613,
      "step": 440
    },
    {
      "epoch": 1.8072289156626506,
      "grad_norm": 0.2406304031610489,
      "learning_rate": 8.196787148594378e-05,
      "loss": 0.0471,
      "step": 450
    },
    {
      "epoch": 1.8473895582329316,
      "grad_norm": 0.9183751344680786,
      "learning_rate": 8.156626506024097e-05,
      "loss": 0.0664,
      "step": 460
    },
    {
      "epoch": 1.8875502008032128,
      "grad_norm": 0.1286589652299881,
      "learning_rate": 8.116465863453816e-05,
      "loss": 0.0458,
      "step": 470
    },
    {
      "epoch": 1.927710843373494,
      "grad_norm": 0.20136843621730804,
      "learning_rate": 8.076305220883534e-05,
      "loss": 0.0418,
      "step": 480
    },
    {
      "epoch": 1.9678714859437751,
      "grad_norm": 0.10740706324577332,
      "learning_rate": 8.036144578313253e-05,
      "loss": 0.03,
      "step": 490
    },
    {
      "epoch": 2.0080321285140563,
      "grad_norm": 0.1058952808380127,
      "learning_rate": 7.995983935742972e-05,
      "loss": 0.0448,
      "step": 500
    },
    {
      "epoch": 2.0481927710843375,
      "grad_norm": 0.24176716804504395,
      "learning_rate": 7.955823293172691e-05,
      "loss": 0.0359,
      "step": 510
    },
    {
      "epoch": 2.0883534136546187,
      "grad_norm": 0.3678503632545471,
      "learning_rate": 7.91566265060241e-05,
      "loss": 0.0317,
      "step": 520
    },
    {
      "epoch": 2.1285140562248994,
      "grad_norm": 0.20136038959026337,
      "learning_rate": 7.875502008032129e-05,
      "loss": 0.0345,
      "step": 530
    },
    {
      "epoch": 2.1686746987951806,
      "grad_norm": 0.4237838089466095,
      "learning_rate": 7.835341365461848e-05,
      "loss": 0.0422,
      "step": 540
    },
    {
      "epoch": 2.208835341365462,
      "grad_norm": 0.841884434223175,
      "learning_rate": 7.795180722891567e-05,
      "loss": 0.0507,
      "step": 550
    },
    {
      "epoch": 2.248995983935743,
      "grad_norm": 0.5050318241119385,
      "learning_rate": 7.755020080321286e-05,
      "loss": 0.0276,
      "step": 560
    },
    {
      "epoch": 2.289156626506024,
      "grad_norm": 0.6458197832107544,
      "learning_rate": 7.714859437751005e-05,
      "loss": 0.0336,
      "step": 570
    },
    {
      "epoch": 2.3293172690763053,
      "grad_norm": 0.3301417827606201,
      "learning_rate": 7.674698795180724e-05,
      "loss": 0.0402,
      "step": 580
    },
    {
      "epoch": 2.3694779116465865,
      "grad_norm": 0.19046242535114288,
      "learning_rate": 7.634538152610443e-05,
      "loss": 0.0407,
      "step": 590
    },
    {
      "epoch": 2.4096385542168672,
      "grad_norm": 0.15934009850025177,
      "learning_rate": 7.594377510040162e-05,
      "loss": 0.0374,
      "step": 600
    },
    {
      "epoch": 2.4497991967871484,
      "grad_norm": 0.3712891936302185,
      "learning_rate": 7.55421686746988e-05,
      "loss": 0.0245,
      "step": 610
    },
    {
      "epoch": 2.4899598393574296,
      "grad_norm": 0.21216151118278503,
      "learning_rate": 7.514056224899598e-05,
      "loss": 0.0464,
      "step": 620
    },
    {
      "epoch": 2.5301204819277108,
      "grad_norm": 0.2159603387117386,
      "learning_rate": 7.473895582329318e-05,
      "loss": 0.033,
      "step": 630
    },
    {
      "epoch": 2.570281124497992,
      "grad_norm": 0.11030252277851105,
      "learning_rate": 7.433734939759036e-05,
      "loss": 0.0305,
      "step": 640
    },
    {
      "epoch": 2.610441767068273,
      "grad_norm": 0.396829217672348,
      "learning_rate": 7.393574297188756e-05,
      "loss": 0.0199,
      "step": 650
    },
    {
      "epoch": 2.6506024096385543,
      "grad_norm": 0.04512975737452507,
      "learning_rate": 7.353413654618474e-05,
      "loss": 0.0253,
      "step": 660
    },
    {
      "epoch": 2.6907630522088355,
      "grad_norm": 0.11919378489255905,
      "learning_rate": 7.313253012048194e-05,
      "loss": 0.0403,
      "step": 670
    },
    {
      "epoch": 2.7309236947791167,
      "grad_norm": 0.7726666331291199,
      "learning_rate": 7.273092369477912e-05,
      "loss": 0.0333,
      "step": 680
    },
    {
      "epoch": 2.7710843373493974,
      "grad_norm": 0.07177244871854782,
      "learning_rate": 7.232931726907632e-05,
      "loss": 0.0291,
      "step": 690
    },
    {
      "epoch": 2.8112449799196786,
      "grad_norm": 0.27916884422302246,
      "learning_rate": 7.19277108433735e-05,
      "loss": 0.0341,
      "step": 700
    },
    {
      "epoch": 2.8514056224899598,
      "grad_norm": 0.8268225789070129,
      "learning_rate": 7.152610441767068e-05,
      "loss": 0.0331,
      "step": 710
    },
    {
      "epoch": 2.891566265060241,
      "grad_norm": 0.14438408613204956,
      "learning_rate": 7.112449799196787e-05,
      "loss": 0.0227,
      "step": 720
    },
    {
      "epoch": 2.931726907630522,
      "grad_norm": 0.09670822322368622,
      "learning_rate": 7.072289156626506e-05,
      "loss": 0.025,
      "step": 730
    },
    {
      "epoch": 2.9718875502008033,
      "grad_norm": 0.0985882431268692,
      "learning_rate": 7.032128514056225e-05,
      "loss": 0.0234,
      "step": 740
    },
    {
      "epoch": 3.0120481927710845,
      "grad_norm": 0.2715780436992645,
      "learning_rate": 6.991967871485944e-05,
      "loss": 0.0213,
      "step": 750
    },
    {
      "epoch": 3.0522088353413657,
      "grad_norm": 0.28619638085365295,
      "learning_rate": 6.951807228915663e-05,
      "loss": 0.0196,
      "step": 760
    },
    {
      "epoch": 3.0923694779116464,
      "grad_norm": 0.3083443343639374,
      "learning_rate": 6.911646586345382e-05,
      "loss": 0.0268,
      "step": 770
    },
    {
      "epoch": 3.1325301204819276,
      "grad_norm": 0.18239940702915192,
      "learning_rate": 6.871485943775101e-05,
      "loss": 0.0166,
      "step": 780
    },
    {
      "epoch": 3.1726907630522088,
      "grad_norm": 0.34872329235076904,
      "learning_rate": 6.83132530120482e-05,
      "loss": 0.022,
      "step": 790
    },
    {
      "epoch": 3.21285140562249,
      "grad_norm": 0.808938205242157,
      "learning_rate": 6.791164658634539e-05,
      "loss": 0.0165,
      "step": 800
    },
    {
      "epoch": 3.253012048192771,
      "grad_norm": 0.5072485208511353,
      "learning_rate": 6.751004016064258e-05,
      "loss": 0.0358,
      "step": 810
    },
    {
      "epoch": 3.2931726907630523,
      "grad_norm": 0.1879962980747223,
      "learning_rate": 6.710843373493975e-05,
      "loss": 0.0192,
      "step": 820
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.16922570765018463,
      "learning_rate": 6.670682730923695e-05,
      "loss": 0.0128,
      "step": 830
    },
    {
      "epoch": 3.3734939759036147,
      "grad_norm": 0.19116787612438202,
      "learning_rate": 6.630522088353413e-05,
      "loss": 0.0207,
      "step": 840
    },
    {
      "epoch": 3.4136546184738954,
      "grad_norm": 0.26532819867134094,
      "learning_rate": 6.590361445783133e-05,
      "loss": 0.0187,
      "step": 850
    },
    {
      "epoch": 3.4538152610441766,
      "grad_norm": 0.1438378244638443,
      "learning_rate": 6.550200803212852e-05,
      "loss": 0.0139,
      "step": 860
    },
    {
      "epoch": 3.4939759036144578,
      "grad_norm": 0.15060514211654663,
      "learning_rate": 6.510040160642571e-05,
      "loss": 0.0137,
      "step": 870
    },
    {
      "epoch": 3.534136546184739,
      "grad_norm": 1.1150715351104736,
      "learning_rate": 6.46987951807229e-05,
      "loss": 0.0083,
      "step": 880
    },
    {
      "epoch": 3.57429718875502,
      "grad_norm": 0.17954625189304352,
      "learning_rate": 6.429718875502009e-05,
      "loss": 0.0143,
      "step": 890
    },
    {
      "epoch": 3.6144578313253013,
      "grad_norm": 1.1485178470611572,
      "learning_rate": 6.389558232931728e-05,
      "loss": 0.0171,
      "step": 900
    },
    {
      "epoch": 3.6546184738955825,
      "grad_norm": 0.45047399401664734,
      "learning_rate": 6.349397590361445e-05,
      "loss": 0.0239,
      "step": 910
    },
    {
      "epoch": 3.694779116465863,
      "grad_norm": 0.09024713188409805,
      "learning_rate": 6.309236947791166e-05,
      "loss": 0.0151,
      "step": 920
    },
    {
      "epoch": 3.734939759036145,
      "grad_norm": 0.038647182285785675,
      "learning_rate": 6.269076305220883e-05,
      "loss": 0.0115,
      "step": 930
    },
    {
      "epoch": 3.7751004016064256,
      "grad_norm": 0.4594738185405731,
      "learning_rate": 6.228915662650604e-05,
      "loss": 0.0118,
      "step": 940
    },
    {
      "epoch": 3.8152610441767068,
      "grad_norm": 0.12905435264110565,
      "learning_rate": 6.188755020080321e-05,
      "loss": 0.022,
      "step": 950
    },
    {
      "epoch": 3.855421686746988,
      "grad_norm": 0.9488304257392883,
      "learning_rate": 6.148594377510041e-05,
      "loss": 0.0236,
      "step": 960
    },
    {
      "epoch": 3.895582329317269,
      "grad_norm": 0.30409470200538635,
      "learning_rate": 6.108433734939759e-05,
      "loss": 0.013,
      "step": 970
    },
    {
      "epoch": 3.9357429718875503,
      "grad_norm": 0.27590280771255493,
      "learning_rate": 6.0682730923694785e-05,
      "loss": 0.0231,
      "step": 980
    },
    {
      "epoch": 3.9759036144578315,
      "grad_norm": 0.6034126877784729,
      "learning_rate": 6.028112449799197e-05,
      "loss": 0.0152,
      "step": 990
    },
    {
      "epoch": 4.016064257028113,
      "grad_norm": 0.3690722584724426,
      "learning_rate": 5.9879518072289164e-05,
      "loss": 0.012,
      "step": 1000
    },
    {
      "epoch": 4.056224899598393,
      "grad_norm": 0.1437559872865677,
      "learning_rate": 5.9477911646586346e-05,
      "loss": 0.0138,
      "step": 1010
    },
    {
      "epoch": 4.096385542168675,
      "grad_norm": 0.038515254855155945,
      "learning_rate": 5.9076305220883535e-05,
      "loss": 0.0091,
      "step": 1020
    },
    {
      "epoch": 4.136546184738956,
      "grad_norm": 0.017356252297759056,
      "learning_rate": 5.8674698795180725e-05,
      "loss": 0.0209,
      "step": 1030
    },
    {
      "epoch": 4.176706827309237,
      "grad_norm": 0.34493300318717957,
      "learning_rate": 5.8273092369477914e-05,
      "loss": 0.0144,
      "step": 1040
    },
    {
      "epoch": 4.216867469879518,
      "grad_norm": 0.027207208797335625,
      "learning_rate": 5.78714859437751e-05,
      "loss": 0.0192,
      "step": 1050
    },
    {
      "epoch": 4.257028112449799,
      "grad_norm": 0.09316883236169815,
      "learning_rate": 5.746987951807229e-05,
      "loss": 0.0147,
      "step": 1060
    },
    {
      "epoch": 4.2971887550200805,
      "grad_norm": 0.039358340203762054,
      "learning_rate": 5.706827309236948e-05,
      "loss": 0.011,
      "step": 1070
    },
    {
      "epoch": 4.337349397590361,
      "grad_norm": 0.07406370341777802,
      "learning_rate": 5.666666666666667e-05,
      "loss": 0.0062,
      "step": 1080
    },
    {
      "epoch": 4.377510040160643,
      "grad_norm": 0.13828614354133606,
      "learning_rate": 5.626506024096386e-05,
      "loss": 0.0168,
      "step": 1090
    },
    {
      "epoch": 4.417670682730924,
      "grad_norm": 0.3330947756767273,
      "learning_rate": 5.586345381526105e-05,
      "loss": 0.0115,
      "step": 1100
    },
    {
      "epoch": 4.457831325301205,
      "grad_norm": 0.14010509848594666,
      "learning_rate": 5.546184738955823e-05,
      "loss": 0.01,
      "step": 1110
    },
    {
      "epoch": 4.497991967871486,
      "grad_norm": 0.06422345340251923,
      "learning_rate": 5.506024096385543e-05,
      "loss": 0.0073,
      "step": 1120
    },
    {
      "epoch": 4.538152610441767,
      "grad_norm": 0.04769006371498108,
      "learning_rate": 5.465863453815261e-05,
      "loss": 0.0095,
      "step": 1130
    },
    {
      "epoch": 4.578313253012048,
      "grad_norm": 0.39416563510894775,
      "learning_rate": 5.4257028112449806e-05,
      "loss": 0.0193,
      "step": 1140
    },
    {
      "epoch": 4.618473895582329,
      "grad_norm": 0.3763667941093445,
      "learning_rate": 5.385542168674699e-05,
      "loss": 0.0183,
      "step": 1150
    },
    {
      "epoch": 4.658634538152611,
      "grad_norm": 0.44258221983909607,
      "learning_rate": 5.3453815261044185e-05,
      "loss": 0.0109,
      "step": 1160
    },
    {
      "epoch": 4.698795180722891,
      "grad_norm": 0.032818008214235306,
      "learning_rate": 5.305220883534137e-05,
      "loss": 0.014,
      "step": 1170
    },
    {
      "epoch": 4.738955823293173,
      "grad_norm": 0.23445093631744385,
      "learning_rate": 5.265060240963856e-05,
      "loss": 0.0145,
      "step": 1180
    },
    {
      "epoch": 4.779116465863454,
      "grad_norm": 0.05525276064872742,
      "learning_rate": 5.2248995983935745e-05,
      "loss": 0.0129,
      "step": 1190
    },
    {
      "epoch": 4.8192771084337345,
      "grad_norm": 0.1274401992559433,
      "learning_rate": 5.184738955823294e-05,
      "loss": 0.0147,
      "step": 1200
    },
    {
      "epoch": 4.859437751004016,
      "grad_norm": 0.09239552170038223,
      "learning_rate": 5.1445783132530124e-05,
      "loss": 0.0052,
      "step": 1210
    },
    {
      "epoch": 4.899598393574297,
      "grad_norm": 0.04652892425656319,
      "learning_rate": 5.1044176706827306e-05,
      "loss": 0.0113,
      "step": 1220
    },
    {
      "epoch": 4.9397590361445785,
      "grad_norm": 0.07636551558971405,
      "learning_rate": 5.06425702811245e-05,
      "loss": 0.0069,
      "step": 1230
    },
    {
      "epoch": 4.979919678714859,
      "grad_norm": 0.8012149930000305,
      "learning_rate": 5.0240963855421685e-05,
      "loss": 0.0237,
      "step": 1240
    },
    {
      "epoch": 5.020080321285141,
      "grad_norm": 0.11616640537977219,
      "learning_rate": 4.983935742971888e-05,
      "loss": 0.0243,
      "step": 1250
    },
    {
      "epoch": 5.0602409638554215,
      "grad_norm": 0.20426449179649353,
      "learning_rate": 4.943775100401606e-05,
      "loss": 0.01,
      "step": 1260
    },
    {
      "epoch": 5.100401606425703,
      "grad_norm": 0.21455571055412292,
      "learning_rate": 4.903614457831325e-05,
      "loss": 0.007,
      "step": 1270
    },
    {
      "epoch": 5.140562248995984,
      "grad_norm": 0.03315037488937378,
      "learning_rate": 4.863453815261044e-05,
      "loss": 0.0048,
      "step": 1280
    },
    {
      "epoch": 5.180722891566265,
      "grad_norm": 0.20422978699207306,
      "learning_rate": 4.823293172690763e-05,
      "loss": 0.0079,
      "step": 1290
    },
    {
      "epoch": 5.220883534136546,
      "grad_norm": 0.11009234935045242,
      "learning_rate": 4.783132530120482e-05,
      "loss": 0.009,
      "step": 1300
    },
    {
      "epoch": 5.261044176706827,
      "grad_norm": 0.11696502566337585,
      "learning_rate": 4.742971887550201e-05,
      "loss": 0.0049,
      "step": 1310
    },
    {
      "epoch": 5.301204819277109,
      "grad_norm": 0.03901469334959984,
      "learning_rate": 4.70281124497992e-05,
      "loss": 0.0166,
      "step": 1320
    },
    {
      "epoch": 5.341365461847389,
      "grad_norm": 0.03349129855632782,
      "learning_rate": 4.662650602409639e-05,
      "loss": 0.0053,
      "step": 1330
    },
    {
      "epoch": 5.381526104417671,
      "grad_norm": 0.27426818013191223,
      "learning_rate": 4.622489959839358e-05,
      "loss": 0.004,
      "step": 1340
    },
    {
      "epoch": 5.421686746987952,
      "grad_norm": 0.06882275640964508,
      "learning_rate": 4.5823293172690766e-05,
      "loss": 0.0042,
      "step": 1350
    },
    {
      "epoch": 5.461847389558233,
      "grad_norm": 0.52886563539505,
      "learning_rate": 4.542168674698795e-05,
      "loss": 0.0148,
      "step": 1360
    },
    {
      "epoch": 5.502008032128514,
      "grad_norm": 0.011637304909527302,
      "learning_rate": 4.502008032128514e-05,
      "loss": 0.0077,
      "step": 1370
    },
    {
      "epoch": 5.542168674698795,
      "grad_norm": 0.6450657844543457,
      "learning_rate": 4.461847389558233e-05,
      "loss": 0.0184,
      "step": 1380
    },
    {
      "epoch": 5.582329317269076,
      "grad_norm": 1.421633243560791,
      "learning_rate": 4.4216867469879516e-05,
      "loss": 0.0162,
      "step": 1390
    },
    {
      "epoch": 5.622489959839357,
      "grad_norm": 0.03103429637849331,
      "learning_rate": 4.3815261044176706e-05,
      "loss": 0.0093,
      "step": 1400
    },
    {
      "epoch": 5.662650602409639,
      "grad_norm": 0.02568615972995758,
      "learning_rate": 4.3413654618473895e-05,
      "loss": 0.0063,
      "step": 1410
    },
    {
      "epoch": 5.7028112449799195,
      "grad_norm": 0.20842388272285461,
      "learning_rate": 4.3012048192771084e-05,
      "loss": 0.0061,
      "step": 1420
    },
    {
      "epoch": 5.742971887550201,
      "grad_norm": 0.4279751777648926,
      "learning_rate": 4.261044176706827e-05,
      "loss": 0.0092,
      "step": 1430
    },
    {
      "epoch": 5.783132530120482,
      "grad_norm": 0.03239402547478676,
      "learning_rate": 4.220883534136546e-05,
      "loss": 0.0049,
      "step": 1440
    },
    {
      "epoch": 5.823293172690763,
      "grad_norm": 0.03743208944797516,
      "learning_rate": 4.180722891566265e-05,
      "loss": 0.0065,
      "step": 1450
    },
    {
      "epoch": 5.863453815261044,
      "grad_norm": 0.7525573372840881,
      "learning_rate": 4.140562248995984e-05,
      "loss": 0.0151,
      "step": 1460
    },
    {
      "epoch": 5.903614457831325,
      "grad_norm": 0.7381136417388916,
      "learning_rate": 4.100401606425703e-05,
      "loss": 0.0171,
      "step": 1470
    },
    {
      "epoch": 5.943775100401607,
      "grad_norm": 0.20716522634029388,
      "learning_rate": 4.060240963855422e-05,
      "loss": 0.019,
      "step": 1480
    },
    {
      "epoch": 5.983935742971887,
      "grad_norm": 0.023733286187052727,
      "learning_rate": 4.020080321285141e-05,
      "loss": 0.0097,
      "step": 1490
    },
    {
      "epoch": 6.024096385542169,
      "grad_norm": 0.4722154140472412,
      "learning_rate": 3.97991967871486e-05,
      "loss": 0.0058,
      "step": 1500
    },
    {
      "epoch": 6.06425702811245,
      "grad_norm": 0.31709569692611694,
      "learning_rate": 3.939759036144579e-05,
      "loss": 0.0071,
      "step": 1510
    },
    {
      "epoch": 6.104417670682731,
      "grad_norm": 0.04816560819745064,
      "learning_rate": 3.8995983935742976e-05,
      "loss": 0.004,
      "step": 1520
    },
    {
      "epoch": 6.144578313253012,
      "grad_norm": 1.1407912969589233,
      "learning_rate": 3.8594377510040166e-05,
      "loss": 0.0136,
      "step": 1530
    },
    {
      "epoch": 6.184738955823293,
      "grad_norm": 0.15391123294830322,
      "learning_rate": 3.8192771084337355e-05,
      "loss": 0.007,
      "step": 1540
    },
    {
      "epoch": 6.224899598393574,
      "grad_norm": 0.056283701211214066,
      "learning_rate": 3.7791164658634544e-05,
      "loss": 0.0064,
      "step": 1550
    },
    {
      "epoch": 6.265060240963855,
      "grad_norm": 0.07769690454006195,
      "learning_rate": 3.7389558232931726e-05,
      "loss": 0.0052,
      "step": 1560
    },
    {
      "epoch": 6.305220883534137,
      "grad_norm": 0.009235331788659096,
      "learning_rate": 3.6987951807228916e-05,
      "loss": 0.003,
      "step": 1570
    },
    {
      "epoch": 6.3453815261044175,
      "grad_norm": 0.01270332932472229,
      "learning_rate": 3.6586345381526105e-05,
      "loss": 0.0106,
      "step": 1580
    },
    {
      "epoch": 6.385542168674699,
      "grad_norm": 0.10949842631816864,
      "learning_rate": 3.6184738955823294e-05,
      "loss": 0.0147,
      "step": 1590
    },
    {
      "epoch": 6.42570281124498,
      "grad_norm": 0.03286124765872955,
      "learning_rate": 3.578313253012048e-05,
      "loss": 0.0035,
      "step": 1600
    },
    {
      "epoch": 6.4658634538152615,
      "grad_norm": 0.0338565818965435,
      "learning_rate": 3.538152610441767e-05,
      "loss": 0.0034,
      "step": 1610
    },
    {
      "epoch": 6.506024096385542,
      "grad_norm": 0.0569310337305069,
      "learning_rate": 3.497991967871486e-05,
      "loss": 0.0079,
      "step": 1620
    },
    {
      "epoch": 6.546184738955823,
      "grad_norm": 0.054306212812662125,
      "learning_rate": 3.457831325301205e-05,
      "loss": 0.005,
      "step": 1630
    },
    {
      "epoch": 6.586345381526105,
      "grad_norm": 0.19887211918830872,
      "learning_rate": 3.417670682730924e-05,
      "loss": 0.0129,
      "step": 1640
    },
    {
      "epoch": 6.626506024096385,
      "grad_norm": 0.02012430876493454,
      "learning_rate": 3.377510040160643e-05,
      "loss": 0.0029,
      "step": 1650
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.03896954655647278,
      "learning_rate": 3.337349397590361e-05,
      "loss": 0.0077,
      "step": 1660
    },
    {
      "epoch": 6.706827309236948,
      "grad_norm": 0.16885234415531158,
      "learning_rate": 3.29718875502008e-05,
      "loss": 0.0146,
      "step": 1670
    },
    {
      "epoch": 6.746987951807229,
      "grad_norm": 0.04594895988702774,
      "learning_rate": 3.257028112449799e-05,
      "loss": 0.0078,
      "step": 1680
    },
    {
      "epoch": 6.78714859437751,
      "grad_norm": 0.017368147149682045,
      "learning_rate": 3.216867469879518e-05,
      "loss": 0.0034,
      "step": 1690
    },
    {
      "epoch": 6.827309236947791,
      "grad_norm": 0.016199836507439613,
      "learning_rate": 3.176706827309237e-05,
      "loss": 0.0064,
      "step": 1700
    },
    {
      "epoch": 6.867469879518072,
      "grad_norm": 0.06714694201946259,
      "learning_rate": 3.136546184738956e-05,
      "loss": 0.0122,
      "step": 1710
    },
    {
      "epoch": 6.907630522088353,
      "grad_norm": 0.06958562135696411,
      "learning_rate": 3.096385542168675e-05,
      "loss": 0.0072,
      "step": 1720
    },
    {
      "epoch": 6.947791164658635,
      "grad_norm": 0.043185871094465256,
      "learning_rate": 3.0562248995983937e-05,
      "loss": 0.0037,
      "step": 1730
    },
    {
      "epoch": 6.9879518072289155,
      "grad_norm": 0.04651818796992302,
      "learning_rate": 3.016064257028113e-05,
      "loss": 0.0046,
      "step": 1740
    },
    {
      "epoch": 7.028112449799197,
      "grad_norm": 0.009555724449455738,
      "learning_rate": 2.975903614457832e-05,
      "loss": 0.0106,
      "step": 1750
    },
    {
      "epoch": 7.068273092369478,
      "grad_norm": 0.10698984563350677,
      "learning_rate": 2.93574297188755e-05,
      "loss": 0.004,
      "step": 1760
    },
    {
      "epoch": 7.108433734939759,
      "grad_norm": 0.020352568477392197,
      "learning_rate": 2.895582329317269e-05,
      "loss": 0.0064,
      "step": 1770
    },
    {
      "epoch": 7.14859437751004,
      "grad_norm": 0.12190116196870804,
      "learning_rate": 2.855421686746988e-05,
      "loss": 0.0048,
      "step": 1780
    },
    {
      "epoch": 7.188755020080321,
      "grad_norm": 0.007631714455783367,
      "learning_rate": 2.815261044176707e-05,
      "loss": 0.0046,
      "step": 1790
    },
    {
      "epoch": 7.228915662650603,
      "grad_norm": 0.027085110545158386,
      "learning_rate": 2.7751004016064258e-05,
      "loss": 0.0119,
      "step": 1800
    },
    {
      "epoch": 7.269076305220883,
      "grad_norm": 0.02726353146135807,
      "learning_rate": 2.7349397590361447e-05,
      "loss": 0.005,
      "step": 1810
    },
    {
      "epoch": 7.309236947791165,
      "grad_norm": 0.03957528620958328,
      "learning_rate": 2.6947791164658636e-05,
      "loss": 0.0106,
      "step": 1820
    },
    {
      "epoch": 7.349397590361446,
      "grad_norm": 0.03976646438241005,
      "learning_rate": 2.6546184738955825e-05,
      "loss": 0.0031,
      "step": 1830
    },
    {
      "epoch": 7.389558232931727,
      "grad_norm": 0.0254521481692791,
      "learning_rate": 2.6144578313253015e-05,
      "loss": 0.0145,
      "step": 1840
    },
    {
      "epoch": 7.429718875502008,
      "grad_norm": 0.12110795080661774,
      "learning_rate": 2.5742971887550204e-05,
      "loss": 0.0052,
      "step": 1850
    },
    {
      "epoch": 7.469879518072289,
      "grad_norm": 0.02233893796801567,
      "learning_rate": 2.534136546184739e-05,
      "loss": 0.0062,
      "step": 1860
    },
    {
      "epoch": 7.51004016064257,
      "grad_norm": 0.060515955090522766,
      "learning_rate": 2.493975903614458e-05,
      "loss": 0.0032,
      "step": 1870
    },
    {
      "epoch": 7.550200803212851,
      "grad_norm": 1.0357204675674438,
      "learning_rate": 2.4538152610441768e-05,
      "loss": 0.0084,
      "step": 1880
    },
    {
      "epoch": 7.590361445783133,
      "grad_norm": 0.0190001018345356,
      "learning_rate": 2.4136546184738957e-05,
      "loss": 0.0109,
      "step": 1890
    },
    {
      "epoch": 7.6305220883534135,
      "grad_norm": 0.010889347642660141,
      "learning_rate": 2.3734939759036147e-05,
      "loss": 0.0029,
      "step": 1900
    },
    {
      "epoch": 7.670682730923695,
      "grad_norm": 0.15844635665416718,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0125,
      "step": 1910
    },
    {
      "epoch": 7.710843373493976,
      "grad_norm": 0.20148621499538422,
      "learning_rate": 2.2931726907630525e-05,
      "loss": 0.0058,
      "step": 1920
    },
    {
      "epoch": 7.7510040160642575,
      "grad_norm": 0.11928816884756088,
      "learning_rate": 2.253012048192771e-05,
      "loss": 0.0044,
      "step": 1930
    },
    {
      "epoch": 7.791164658634538,
      "grad_norm": 0.053826551884412766,
      "learning_rate": 2.21285140562249e-05,
      "loss": 0.0028,
      "step": 1940
    },
    {
      "epoch": 7.831325301204819,
      "grad_norm": 0.061694320291280746,
      "learning_rate": 2.172690763052209e-05,
      "loss": 0.004,
      "step": 1950
    },
    {
      "epoch": 7.871485943775101,
      "grad_norm": 0.04919566959142685,
      "learning_rate": 2.132530120481928e-05,
      "loss": 0.0112,
      "step": 1960
    },
    {
      "epoch": 7.911646586345381,
      "grad_norm": 0.10250387340784073,
      "learning_rate": 2.0923694779116468e-05,
      "loss": 0.0033,
      "step": 1970
    },
    {
      "epoch": 7.951807228915663,
      "grad_norm": 0.02862020768225193,
      "learning_rate": 2.0522088353413654e-05,
      "loss": 0.0042,
      "step": 1980
    },
    {
      "epoch": 7.991967871485944,
      "grad_norm": 0.02175704948604107,
      "learning_rate": 2.0120481927710843e-05,
      "loss": 0.0063,
      "step": 1990
    },
    {
      "epoch": 8.032128514056225,
      "grad_norm": 0.013178159482777119,
      "learning_rate": 1.9718875502008032e-05,
      "loss": 0.0047,
      "step": 2000
    },
    {
      "epoch": 8.072289156626505,
      "grad_norm": 0.16432930529117584,
      "learning_rate": 1.931726907630522e-05,
      "loss": 0.0041,
      "step": 2010
    },
    {
      "epoch": 8.112449799196787,
      "grad_norm": 0.33876538276672363,
      "learning_rate": 1.891566265060241e-05,
      "loss": 0.009,
      "step": 2020
    },
    {
      "epoch": 8.152610441767068,
      "grad_norm": 0.20591114461421967,
      "learning_rate": 1.8514056224899596e-05,
      "loss": 0.0025,
      "step": 2030
    },
    {
      "epoch": 8.19277108433735,
      "grad_norm": 0.04350712150335312,
      "learning_rate": 1.811244979919679e-05,
      "loss": 0.0053,
      "step": 2040
    },
    {
      "epoch": 8.23293172690763,
      "grad_norm": 0.8948339223861694,
      "learning_rate": 1.7710843373493978e-05,
      "loss": 0.0114,
      "step": 2050
    },
    {
      "epoch": 8.273092369477911,
      "grad_norm": 0.290385901927948,
      "learning_rate": 1.7309236947791167e-05,
      "loss": 0.0225,
      "step": 2060
    },
    {
      "epoch": 8.313253012048193,
      "grad_norm": 0.05442654341459274,
      "learning_rate": 1.6907630522088357e-05,
      "loss": 0.0087,
      "step": 2070
    },
    {
      "epoch": 8.353413654618475,
      "grad_norm": 0.01176400762051344,
      "learning_rate": 1.6506024096385542e-05,
      "loss": 0.0033,
      "step": 2080
    },
    {
      "epoch": 8.393574297188755,
      "grad_norm": 0.0375906378030777,
      "learning_rate": 1.6104417670682732e-05,
      "loss": 0.0031,
      "step": 2090
    },
    {
      "epoch": 8.433734939759036,
      "grad_norm": 0.058879151940345764,
      "learning_rate": 1.570281124497992e-05,
      "loss": 0.0027,
      "step": 2100
    },
    {
      "epoch": 8.473895582329318,
      "grad_norm": 0.024047303944826126,
      "learning_rate": 1.530120481927711e-05,
      "loss": 0.0084,
      "step": 2110
    },
    {
      "epoch": 8.514056224899598,
      "grad_norm": 0.30300799012184143,
      "learning_rate": 1.48995983935743e-05,
      "loss": 0.0109,
      "step": 2120
    },
    {
      "epoch": 8.55421686746988,
      "grad_norm": 0.6735260486602783,
      "learning_rate": 1.4497991967871485e-05,
      "loss": 0.004,
      "step": 2130
    },
    {
      "epoch": 8.594377510040161,
      "grad_norm": 0.020426947623491287,
      "learning_rate": 1.4096385542168674e-05,
      "loss": 0.0047,
      "step": 2140
    },
    {
      "epoch": 8.634538152610443,
      "grad_norm": 0.3897150754928589,
      "learning_rate": 1.3694779116465864e-05,
      "loss": 0.0093,
      "step": 2150
    },
    {
      "epoch": 8.674698795180722,
      "grad_norm": 0.019139084964990616,
      "learning_rate": 1.3293172690763053e-05,
      "loss": 0.0023,
      "step": 2160
    },
    {
      "epoch": 8.714859437751004,
      "grad_norm": 0.024438196793198586,
      "learning_rate": 1.2891566265060242e-05,
      "loss": 0.0037,
      "step": 2170
    },
    {
      "epoch": 8.755020080321286,
      "grad_norm": 0.030319329351186752,
      "learning_rate": 1.248995983935743e-05,
      "loss": 0.0029,
      "step": 2180
    },
    {
      "epoch": 8.795180722891565,
      "grad_norm": 0.15567703545093536,
      "learning_rate": 1.2088353413654619e-05,
      "loss": 0.0062,
      "step": 2190
    },
    {
      "epoch": 8.835341365461847,
      "grad_norm": 0.03405065834522247,
      "learning_rate": 1.1686746987951808e-05,
      "loss": 0.0024,
      "step": 2200
    },
    {
      "epoch": 8.875502008032129,
      "grad_norm": 0.772620439529419,
      "learning_rate": 1.1285140562248997e-05,
      "loss": 0.0052,
      "step": 2210
    },
    {
      "epoch": 8.91566265060241,
      "grad_norm": 0.12610456347465515,
      "learning_rate": 1.0883534136546185e-05,
      "loss": 0.0056,
      "step": 2220
    },
    {
      "epoch": 8.95582329317269,
      "grad_norm": 0.027354247868061066,
      "learning_rate": 1.0481927710843374e-05,
      "loss": 0.0063,
      "step": 2230
    },
    {
      "epoch": 8.995983935742972,
      "grad_norm": 0.03759048134088516,
      "learning_rate": 1.0080321285140562e-05,
      "loss": 0.0066,
      "step": 2240
    },
    {
      "epoch": 9.036144578313253,
      "grad_norm": 0.08246171474456787,
      "learning_rate": 9.67871485943775e-06,
      "loss": 0.0039,
      "step": 2250
    },
    {
      "epoch": 9.076305220883533,
      "grad_norm": 0.012782983481884003,
      "learning_rate": 9.27710843373494e-06,
      "loss": 0.008,
      "step": 2260
    },
    {
      "epoch": 9.116465863453815,
      "grad_norm": 0.05471457168459892,
      "learning_rate": 8.87550200803213e-06,
      "loss": 0.004,
      "step": 2270
    },
    {
      "epoch": 9.156626506024097,
      "grad_norm": 0.01529618352651596,
      "learning_rate": 8.473895582329319e-06,
      "loss": 0.0054,
      "step": 2280
    },
    {
      "epoch": 9.196787148594378,
      "grad_norm": 0.9484298825263977,
      "learning_rate": 8.072289156626506e-06,
      "loss": 0.0097,
      "step": 2290
    },
    {
      "epoch": 9.236947791164658,
      "grad_norm": 0.022590795531868935,
      "learning_rate": 7.670682730923695e-06,
      "loss": 0.0024,
      "step": 2300
    },
    {
      "epoch": 9.27710843373494,
      "grad_norm": 0.08827123045921326,
      "learning_rate": 7.2690763052208845e-06,
      "loss": 0.0042,
      "step": 2310
    },
    {
      "epoch": 9.317269076305221,
      "grad_norm": 0.011456707492470741,
      "learning_rate": 6.867469879518072e-06,
      "loss": 0.0082,
      "step": 2320
    },
    {
      "epoch": 9.357429718875501,
      "grad_norm": 0.009425816126167774,
      "learning_rate": 6.465863453815261e-06,
      "loss": 0.0035,
      "step": 2330
    },
    {
      "epoch": 9.397590361445783,
      "grad_norm": 0.47055163979530334,
      "learning_rate": 6.0642570281124505e-06,
      "loss": 0.0172,
      "step": 2340
    },
    {
      "epoch": 9.437751004016064,
      "grad_norm": 0.12915095686912537,
      "learning_rate": 5.662650602409639e-06,
      "loss": 0.015,
      "step": 2350
    },
    {
      "epoch": 9.477911646586346,
      "grad_norm": 0.1218683049082756,
      "learning_rate": 5.261044176706827e-06,
      "loss": 0.0021,
      "step": 2360
    },
    {
      "epoch": 9.518072289156626,
      "grad_norm": 0.027184929698705673,
      "learning_rate": 4.8594377510040165e-06,
      "loss": 0.0105,
      "step": 2370
    },
    {
      "epoch": 9.558232931726907,
      "grad_norm": 0.08872534334659576,
      "learning_rate": 4.457831325301205e-06,
      "loss": 0.0065,
      "step": 2380
    },
    {
      "epoch": 9.598393574297189,
      "grad_norm": 0.013547660782933235,
      "learning_rate": 4.056224899598394e-06,
      "loss": 0.0058,
      "step": 2390
    },
    {
      "epoch": 9.638554216867469,
      "grad_norm": 0.014878672547638416,
      "learning_rate": 3.6546184738955825e-06,
      "loss": 0.0024,
      "step": 2400
    },
    {
      "epoch": 9.67871485943775,
      "grad_norm": 0.662963330745697,
      "learning_rate": 3.253012048192771e-06,
      "loss": 0.0034,
      "step": 2410
    },
    {
      "epoch": 9.718875502008032,
      "grad_norm": 0.009639382362365723,
      "learning_rate": 2.85140562248996e-06,
      "loss": 0.0019,
      "step": 2420
    },
    {
      "epoch": 9.759036144578314,
      "grad_norm": 0.027606241405010223,
      "learning_rate": 2.4497991967871484e-06,
      "loss": 0.005,
      "step": 2430
    },
    {
      "epoch": 9.799196787148594,
      "grad_norm": 1.1460511684417725,
      "learning_rate": 2.0481927710843377e-06,
      "loss": 0.0062,
      "step": 2440
    },
    {
      "epoch": 9.839357429718875,
      "grad_norm": 0.14809633791446686,
      "learning_rate": 1.646586345381526e-06,
      "loss": 0.003,
      "step": 2450
    },
    {
      "epoch": 9.879518072289157,
      "grad_norm": 0.020319528877735138,
      "learning_rate": 1.2449799196787148e-06,
      "loss": 0.0028,
      "step": 2460
    },
    {
      "epoch": 9.919678714859439,
      "grad_norm": 0.029056090861558914,
      "learning_rate": 8.433734939759036e-07,
      "loss": 0.0068,
      "step": 2470
    },
    {
      "epoch": 9.959839357429718,
      "grad_norm": 0.011855321936309338,
      "learning_rate": 4.4176706827309234e-07,
      "loss": 0.0049,
      "step": 2480
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.24908718466758728,
      "learning_rate": 4.016064257028113e-08,
      "loss": 0.0125,
      "step": 2490
    }
  ],
  "logging_steps": 10,
  "max_steps": 2490,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1321731008778240.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
