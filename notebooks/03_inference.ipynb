{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64276ea",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Run inference on test images using the fine-tuned LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA model from ../models/layoutlmv3-lora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ryanznie/Desktop/work/invoice-ner/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:1625: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m     words = item[\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     82\u001b[39m     boxes = item[\u001b[33m'\u001b[39m\u001b[33mbboxes\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     predicted_labels, invoice_number = \u001b[43mpredict_invoice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../models/layoutlmv3-lora\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/layoutlmv3-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     results.append({\n\u001b[32m     94\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: item[\u001b[33m'\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     95\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpredicted_labels\u001b[39m\u001b[33m\"\u001b[39m: predicted_labels,\n\u001b[32m     96\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minvoice_number\u001b[39m\u001b[33m\"\u001b[39m: invoice_number\n\u001b[32m     97\u001b[39m     })\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Save results\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mpredict_invoice\u001b[39m\u001b[34m(image_path, words, boxes, model_path, base_model, num_labels)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03mRun inference on a single invoice\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load processor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m processor = \u001b[43mLayoutLMv3Processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_ocr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Load base model + LoRA adapter\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading LoRA model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/work/invoice-ner/.venv/lib/python3.11/site-packages/transformers/processing_utils.py:1394\u001b[39m, in \u001b[36mProcessorMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1392\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m args = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1395\u001b[39m processor_dict, kwargs = \u001b[38;5;28mcls\u001b[39m.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_args_and_dict(args, processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/work/invoice-ner/.venv/lib/python3.11/site-packages/transformers/processing_utils.py:1453\u001b[39m, in \u001b[36mProcessorMixin._get_arguments_from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1451\u001b[39m         attribute_class = \u001b[38;5;28mcls\u001b[39m.get_possibly_dynamic_module(class_name)\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m     args.append(\u001b[43mattribute_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/work/invoice-ner/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2097\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2094\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2095\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2097\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2100\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/work/invoice-ner/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2343\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2341\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2342\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2343\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2345\u001b[39m     logger.info(\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2348\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/work/invoice-ner/.venv/lib/python3.11/site-packages/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py:143\u001b[39m, in \u001b[36mLayoutLMv3TokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, tokenizer_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, trim_offsets, cls_token_box, sep_token_box, pad_token_box, pad_token_label, only_label_first_subword, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    123\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m     **kwargs,\n\u001b[32m    142\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrim_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrim_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token_box\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token_box\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token_box\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token_box\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_box\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token_box\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_label_first_subword\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_label_first_subword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     tokenizer_component = \u001b[33m\"\u001b[39m\u001b[33mpost_processor\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m     tokenizer_component_instance = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.backend_tokenizer, tokenizer_component, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/work/invoice-ner/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:117\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m     fast_tokenizer = copy.deepcopy(tokenizer_object)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     fast_tokenizer = \u001b[43mTokenizerFast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer:\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[32m    120\u001b[39m     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "def predict_invoice(\n",
    "    image_path: str,\n",
    "    words: List[str],\n",
    "    boxes: List[List[int]],\n",
    "    model_path: str = \"models/layoutlmv3-lora-invoice-number\",\n",
    "    base_model: str = \"microsoft/layoutlmv3-base\",\n",
    "    num_labels: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Run inference on a single invoice\n",
    "    \"\"\"\n",
    "    # Load processor\n",
    "    processor = LayoutLMv3Processor.from_pretrained(model_path, apply_ocr=False)\n",
    "    \n",
    "    # Load base model + LoRA adapter\n",
    "    print(f\"Loading LoRA model from {model_path}...\")\n",
    "    base = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "        base_model,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base, model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Process\n",
    "    encoding = processor(\n",
    "        image,\n",
    "        words,\n",
    "        boxes=boxes,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    # Convert to labels\n",
    "    predicted_labels = []\n",
    "    invoice_tokens = []\n",
    "    \n",
    "    token_boxes = encoding.bbox[0].tolist()\n",
    "    word_ids = encoding.word_ids(0)\n",
    "    \n",
    "    prev_word_idx = None\n",
    "    for pred, box, word_idx in zip(predictions[0].tolist(), token_boxes, word_ids):\n",
    "        if box != [0, 0, 0, 0] and word_idx is not None:\n",
    "            label = model.config.id2label[pred]\n",
    "            if word_idx != prev_word_idx:\n",
    "                predicted_labels.append(label)\n",
    "                if label.startswith('B-INVOICE') or label.startswith('I-INVOICE'):\n",
    "                    invoice_tokens.append(words[word_idx])\n",
    "                prev_word_idx = word_idx\n",
    "    \n",
    "    invoice_number = ' '.join(invoice_tokens) if invoice_tokens else None\n",
    "    return predicted_labels, invoice_number\n",
    "\n",
    "\n",
    "# --- Inference on test.json ---\n",
    "test_json_path = \"../data/SROIE2019/test/test.json\"\n",
    "\n",
    "with open(test_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in test_data:\n",
    "    file_path = f\"../data/SROIE2019/test/img/{item['file']}\"\n",
    "    words = item['words']\n",
    "    boxes = item['bboxes']\n",
    "    \n",
    "    predicted_labels, invoice_number = predict_invoice(\n",
    "        image_path=file_path,\n",
    "        words=words,\n",
    "        boxes=boxes,\n",
    "        model_path=\"../models/layoutlmv3-lora-invoice-number\",\n",
    "        base_model=\"microsoft/layoutlmv3-base\",\n",
    "        num_labels=3\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"file\": item['file'],\n",
    "        \"predicted_labels\": predicted_labels,\n",
    "        \"invoice_number\": invoice_number\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "with open(\"../data/SROIE2019/test/predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Inference complete. Predictions saved to predictions.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcbb247",
   "metadata": {},
   "source": [
    "## Inference on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7e8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                 BBOX                 PREDICTION\n",
      "------------------------------------------------------------\n",
      "SYARIKAT             [92, 111, 331, 137] LABEL_0\n",
      "PERNIAGAAN           [361, 111, 661, 137] LABEL_0\n",
      "GIN                  [691, 111, 781, 137] LABEL_0\n",
      "KEE                  [812, 111, 902, 137] LABEL_0\n",
      "(                    [408, 142, 426, 167] LABEL_0\n",
      "81109                [426, 142, 523, 167] LABEL_0\n",
      "-                    [523, 142, 542, 167] LABEL_0\n",
      "A                    [542, 142, 561, 167] LABEL_0\n",
      ")                    [561, 142, 581, 167] LABEL_0\n",
      "NO                   [255, 170, 295, 190] LABEL_0\n",
      "290,                 [314, 170, 395, 190] LABEL_0\n",
      "JALAN                [415, 170, 515, 190] LABEL_0\n",
      "AIR                  [534, 170, 595, 190] LABEL_0\n",
      "PANAS,               [615, 170, 735, 190] LABEL_0\n",
      "SETAPAK,             [403, 191, 588, 216] LABEL_0\n",
      "53200,               [283, 220, 418, 241] LABEL_0\n",
      "KUALA                [441, 220, 554, 241] LABEL_0\n",
      "LUMPUR               [576, 220, 712, 241] LABEL_0\n",
      "TEL                  [334, 242, 392, 263] LABEL_0\n",
      ":                    [411, 242, 430, 263] LABEL_0\n",
      "03                   [450, 242, 488, 263] LABEL_0\n",
      "-                    [488, 242, 508, 263] LABEL_0\n",
      "40210276             [508, 242, 663, 263] LABEL_0\n",
      "GST                  [299, 276, 356, 294] LABEL_0\n",
      "ID                   [375, 276, 414, 294] LABEL_0\n",
      ":                    [433, 276, 453, 294] LABEL_0\n",
      "000750673920         [472, 276, 703, 294] LABEL_0\n",
      "SIMPLIFIED           [244, 302, 471, 322] LABEL_0\n",
      "TAX                  [495, 302, 562, 322] LABEL_0\n",
      "INVOICE              [585, 302, 745, 322] LABEL_0\n",
      "CASH                 [41, 341, 154, 362] LABEL_0\n",
      "DATE                 [593, 366, 671, 393] LABEL_0\n",
      ":                    [671, 366, 690, 393] LABEL_0\n",
      "25                   [709, 366, 748, 393] LABEL_0\n",
      "/                    [748, 366, 767, 393] LABEL_0\n",
      "01                   [767, 366, 806, 393] LABEL_0\n",
      "/                    [806, 366, 825, 393] LABEL_0\n",
      "2018                 [825, 366, 904, 393] LABEL_0\n",
      ":                    [273, 369, 296, 388] LABEL_0\n",
      "CS00012944           [319, 369, 547, 388] LABEL_1\n",
      "DOC                  [48, 373, 114, 392] LABEL_0\n",
      "NO                   [137, 373, 182, 392] LABEL_0\n",
      "TIME                 [597, 393, 675, 418] LABEL_0\n",
      ":                    [675, 393, 696, 418] LABEL_0\n",
      "14                   [715, 393, 755, 418] LABEL_0\n",
      ":                    [755, 393, 774, 418] LABEL_0\n",
      "29                   [774, 393, 814, 418] LABEL_0\n",
      ":                    [814, 393, 834, 418] LABEL_0\n",
      "00                   [834, 393, 874, 418] LABEL_0\n",
      "CASHIER              [44, 398, 184, 419] LABEL_0\n",
      ":                    [283, 400, 307, 418] LABEL_0\n",
      "USER                 [331, 400, 428, 418] LABEL_0\n",
      "REF                  [599, 421, 646, 444] LABEL_0\n",
      ".                    [646, 421, 661, 444] LABEL_0\n",
      ":                    [677, 421, 693, 444] LABEL_0\n",
      "SALESPERSON          [47, 425, 282, 443] LABEL_0\n",
      ":                    [282, 425, 303, 443] LABEL_0\n",
      "S                    [500, 469, 518, 491] LABEL_0\n",
      "/                    [518, 469, 537, 491] LABEL_0\n",
      "PRICE                [537, 469, 629, 491] LABEL_0\n",
      "QTY                  [369, 470, 438, 492] LABEL_0\n",
      "TAX                  [860, 470, 929, 491] LABEL_0\n",
      "AMOUNT               [675, 471, 825, 491] LABEL_0\n",
      "ITEM                 [44, 471, 124, 490] LABEL_0\n",
      "3                    [382, 495, 416, 513] LABEL_0\n",
      "190                  [707, 495, 764, 516] LABEL_0\n",
      ".                    [764, 495, 784, 516] LABEL_0\n",
      "80                   [784, 495, 822, 516] LABEL_0\n",
      "1007                 [48, 497, 138, 516] LABEL_0\n",
      "SR                   [862, 497, 922, 515] LABEL_0\n",
      "63                   [509, 498, 552, 517] LABEL_0\n",
      ".                    [552, 498, 573, 517] LABEL_0\n",
      "60                   [573, 498, 617, 517] LABEL_0\n",
      "12MM                 [50, 521, 140, 542] LABEL_0\n",
      "4                    [163, 521, 185, 542] LABEL_0\n",
      "/                    [185, 521, 208, 542] LABEL_0\n",
      "8B                   [208, 521, 254, 542] LABEL_0\n",
      "PLYWOOD              [277, 521, 436, 542] LABEL_0\n",
      "190                  [707, 547, 767, 564] LABEL_0\n",
      ".                    [767, 547, 787, 564] LABEL_0\n",
      "80                   [787, 547, 828, 564] LABEL_0\n",
      "TOTAL                [140, 549, 221, 571] LABEL_0\n",
      "QTY                  [237, 549, 285, 571] LABEL_0\n",
      ":                    [285, 549, 301, 571] LABEL_0\n",
      "3                    [382, 549, 416, 565] LABEL_0\n",
      "180                  [705, 585, 763, 603] LABEL_0\n",
      ".                    [763, 585, 783, 603] LABEL_0\n",
      "00                   [783, 585, 822, 603] LABEL_0\n",
      "TOTAL                [138, 587, 224, 605] LABEL_0\n",
      "SALES                [241, 587, 328, 605] LABEL_0\n",
      "(                    [345, 587, 363, 605] LABEL_0\n",
      "EXCLUDING            [363, 587, 519, 605] LABEL_0\n",
      "GST                  [537, 587, 588, 605] LABEL_0\n",
      ")                    [588, 587, 605, 605] LABEL_0\n",
      ":                    [622, 587, 641, 605] LABEL_0\n",
      "DISCOUNT             [452, 612, 598, 633] LABEL_0\n",
      ":                    [616, 612, 634, 633] LABEL_0\n",
      "0                    [735, 614, 756, 632] LABEL_0\n",
      ".                    [756, 614, 777, 632] LABEL_0\n",
      "00                   [777, 614, 820, 632] LABEL_0\n",
      "10                   [728, 643, 763, 663] LABEL_0\n",
      ".                    [763, 643, 781, 663] LABEL_0\n",
      "80                   [781, 643, 818, 663] LABEL_0\n",
      "TOTAL                [428, 645, 523, 660] LABEL_0\n",
      "GST                  [542, 645, 599, 660] LABEL_0\n",
      ":                    [618, 645, 638, 660] LABEL_0\n",
      "ROUNDING             [442, 670, 595, 692] LABEL_0\n",
      ":                    [614, 670, 633, 692] LABEL_0\n",
      "0                    [742, 672, 760, 690] LABEL_0\n",
      ".                    [760, 672, 779, 690] LABEL_0\n",
      "00                   [779, 672, 818, 690] LABEL_0\n",
      "TOTAL                [74, 704, 161, 726] LABEL_0\n",
      "SALES                [178, 704, 266, 726] LABEL_0\n",
      "(                    [283, 704, 301, 726] LABEL_0\n",
      "INCLUSIVE            [301, 704, 458, 726] LABEL_0\n",
      "OF                   [476, 704, 511, 726] LABEL_0\n",
      "GST                  [529, 704, 582, 726] LABEL_0\n",
      ")                    [582, 704, 599, 726] LABEL_0\n",
      ":                    [616, 704, 634, 726] LABEL_0\n",
      "190                  [703, 705, 765, 722] LABEL_0\n",
      ".                    [765, 705, 786, 722] LABEL_0\n",
      "80                   [786, 705, 828, 722] LABEL_0\n",
      "CASH                 [493, 736, 589, 757] LABEL_0\n",
      ":                    [613, 736, 638, 757] LABEL_0\n",
      "190                  [705, 736, 765, 756] LABEL_0\n",
      ".                    [765, 736, 785, 756] LABEL_0\n",
      "80                   [785, 736, 825, 756] LABEL_0\n",
      "0                    [742, 762, 761, 783] LABEL_0\n",
      ".                    [761, 762, 781, 783] LABEL_0\n",
      "00                   [781, 762, 822, 783] LABEL_0\n",
      "CHANGE               [456, 763, 595, 786] LABEL_0\n",
      ":                    [617, 763, 641, 786] LABEL_0\n",
      "GST                  [110, 813, 193, 834] LABEL_0\n",
      "SUMMARY              [220, 813, 414, 834] LABEL_0\n",
      "%                    [375, 845, 419, 867] LABEL_0\n",
      "TAX                  [726, 847, 793, 867] LABEL_0\n",
      "(                    [793, 847, 816, 867] LABEL_0\n",
      "RM                   [816, 847, 861, 867] LABEL_0\n",
      ")                    [861, 847, 885, 867] LABEL_0\n",
      "TAX                  [120, 848, 185, 869] LABEL_0\n",
      "CODE                 [207, 848, 295, 869] LABEL_0\n",
      "AMT                  [497, 848, 566, 866] LABEL_0\n",
      "(                    [566, 848, 588, 866] LABEL_0\n",
      "RM                   [588, 848, 634, 866] LABEL_0\n",
      ")                    [634, 848, 658, 866] LABEL_0\n",
      "SR                   [124, 870, 198, 893] LABEL_0\n",
      "10                   [781, 870, 824, 891] LABEL_0\n",
      ".                    [824, 870, 846, 891] LABEL_0\n",
      "80                   [846, 870, 890, 891] LABEL_0\n",
      "180                  [530, 871, 599, 892] LABEL_0\n",
      ".                    [599, 871, 622, 892] LABEL_0\n",
      "00                   [622, 871, 669, 892] LABEL_0\n",
      "6                    [373, 872, 419, 889] LABEL_0\n",
      "180                  [533, 901, 593, 926] LABEL_0\n",
      ".                    [593, 901, 614, 926] LABEL_0\n",
      "00                   [614, 901, 655, 926] LABEL_0\n",
      "TOTAL                [317, 902, 408, 925] LABEL_0\n",
      ":                    [426, 902, 444, 925] LABEL_0\n",
      "10                   [783, 902, 823, 921] LABEL_0\n",
      ".                    [823, 902, 845, 921] LABEL_0\n",
      "80                   [845, 902, 887, 921] LABEL_0\n",
      "GOODS                [97, 951, 193, 976] LABEL_0\n",
      "SOLD                 [212, 951, 288, 976] LABEL_0\n",
      "ARE                  [308, 951, 365, 976] LABEL_0\n",
      "NOT                  [384, 951, 442, 976] LABEL_0\n",
      "RETURNABLE,          [461, 951, 672, 976] LABEL_0\n",
      "THANK                [691, 951, 787, 976] LABEL_0\n",
      "YOU                  [806, 951, 864, 976] LABEL_0\n",
      ".                    [864, 951, 883, 976] LABEL_0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Config ---\n",
    "file_to_test = \"X51005675104.jpg\" \n",
    "model_path = \"../models/layoutlmv3-lora-invoice-number\"\n",
    "base_model = \"microsoft/layoutlmv3-base\"\n",
    "num_labels = 3\n",
    "\n",
    "# --- Load test data ---\n",
    "test_json_path = \"../data/SROIE2019/test/test.json\"\n",
    "with open(test_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Find the file\n",
    "item = next(d for d in test_data if d[\"file\"] == file_to_test)\n",
    "words = item[\"words\"]\n",
    "boxes = item[\"bboxes\"]\n",
    "image_path = f\"../data/SROIE2019/test/img/{file_to_test}\"\n",
    "\n",
    "# --- Load processor and model ---\n",
    "processor = LayoutLMv3Processor.from_pretrained(model_path, apply_ocr=False)\n",
    "base = LayoutLMv3ForTokenClassification.from_pretrained(base_model, num_labels=num_labels)\n",
    "model = PeftModel.from_pretrained(base, model_path)\n",
    "model.eval()\n",
    "\n",
    "# --- Load image and encode ---\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "encoding = processor(\n",
    "    image,\n",
    "    words,\n",
    "    boxes=boxes,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# --- Predict ---\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "# --- Print words, bbox, prediction ---\n",
    "token_boxes = encoding.bbox[0].tolist()\n",
    "word_ids = encoding.word_ids(0)\n",
    "prev_word_idx = None\n",
    "\n",
    "print(f\"{'WORD':20} {'BBOX':20} {'PREDICTION'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for pred, box, word_idx in zip(predictions[0].tolist(), token_boxes, word_ids):\n",
    "    if box != [0, 0, 0, 0] and word_idx is not None and word_idx != prev_word_idx:\n",
    "        label = model.config.id2label[pred]\n",
    "        print(f\"{words[word_idx]:20} {boxes[word_idx]} {label}\")\n",
    "        prev_word_idx = word_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef150f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invoice-ner (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
